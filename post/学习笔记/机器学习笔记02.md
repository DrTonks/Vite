---
next:  
    text: '学习笔记 | 模块化开发'
    link: '/学习笔记/模块化开发.md'
prev:
    text: '学习笔记 | ECMAScript规范学习03'
    link: '/学习笔记/ECMAScript规范学习03.md'
---
# 机器学习：学习记录（ML02）

###  Q&A

### <font color="red">1.什么是偏导数？梯度和偏导数有什么联系？</font>

​	**我与偏导的第一次接触始于高中的一道双变量求最值的题目。冥思苦想许久而一筹莫展之际，我突发奇想：“能不能只逮着一个未知数导？”——那时候老师说，这就是偏导。<br>**

​	随着对“机器学习”的了解逐渐加深，我对它也有了更高层次的理解：它是一个用来衡量函数对于某个元素变化的敏感程度的工具。虽然它对我来说一直都是工具，但是从高中的“做题”工具转化为了帮助我理解机器学习模型的工具，也算是完成了一次进化。



- #### **什么是偏导数**？

  让我们假设一个关于x，y双变量的函数
  $$
  z=f(x,y)=x^2+y^2
  $$

  其中，关于x的偏导数
  $$
  \frac{dz}{dx}=\frac{df(x,y)}{dx}=\lim_{\Delta x\to 0}\frac{f(x+\Delta x,y)-f(x,y)}{\Delta x}
  $$
  假设中，该函数关于x的偏导数就为

$$
\frac{dz}{dx}=2x
$$

- #### **梯度和偏导数有什么联系？**

  首先我们需要解决一个问题：梯度是什么？

  假定一个具有多个自变量的光滑函数：
  $$
  g(x_0,x_1,x_2,...,x_n)
  $$
  其梯度（gradient）定义如下：
  $$
  \nabla g
  (x_0,x_1,...,x_n)=(\frac{\partial g}{\partial x_0},\frac{\partial g}{\partial x_1},...,\frac{\partial g}{\partial x_n})
  $$
  单看形式，不难发现其梯度是由该函数的所有偏导数组成的向量，而对于一个单变量函数而言，梯度就是导数。

  我们知道，偏导数表示的是该函数对于某一个变量而言的变化率。那么作为**偏导数的集合**，**梯度这一向量指向的方向就代表了函数值变化最快的方向。**

  也就是说，**对于一个多变量函数，梯度包含了函数对每个变量的偏导数，而函数在变量空间的某一点处，沿着梯度方向有着最大的变化率。**此外，我了解到梯度的模长就是变化率的最大值，真是太神奇了
  
  

------

### <font color="red">2.**梯度下降**是什么？**反向传播**是什么？</font>

​	我们曾在高中学习过“线性回归”：为了找到最合适、最接近一条未知函数的直线/曲线，我们需要通过“回归”来确定最适合的斜率/参数。**这个过程与我所了解到的“梯度下降”似乎有异曲同工之妙，我也愿意相信它就是我接下来将要学习的模块中的一部分。**

- #### **什么是梯度下降？**


​	在机器学习中，一个算法里不同的参数会产生不同的拟合曲线，也就意味着不同的误差值。沿用上文提到的“线性回归”以方便理解：我们需要在许许多多的可选择的函数中挑选一个误差最小、最贴合原函数的直线，实现对未来数据的预测。

​	

​	高中时期，我们使用e来命名残差，或者说“误差”：
$$
e_k=实际值_k-预测值_k
$$
​	为了尽可能减小误差、以及定义所有数据的误差，最简单的办法就是求平方误差的总和,即
$$
C=\sum_{k=1}^{n}   e_k^2
$$
​	为了使C达到最小，我们需要对其进行偏导然后求得最值，偏导的对象分别为回归直线方程的斜率和截距，由于上文解释了偏导用法，这里不多赘叙。



​	其实，C 函数就是在机器学习中常用的一种损失函数。损失函数是一个自变量为算法参数，函数值为误差值的函数，本质上是计算预测值和真实值的差距的函数。

​	这个过程的核心在于最小化损失函数，也就是**梯度下降**：寻找误差值最小的所对应的那一个参数。也就是所谓的“将函数图像看作斜坡，沿着坡度最陡的方向一步一步下降”。

​	**<font color="orange">在更深层次的学习中，我们可能遇到参数极多的函数，用具体的式子去求梯度可能会遭遇极其棘手的计算难题，这时我们就要用到反向传播法。</font>**

- #### **什么是反向传播？**

​	最简单的理解是，反向传播法通过不断更新参数来最小化误差。

​	例如：继续沿用上文设定，定义一个损失函数C
$$
C=\sum_{k=1}^{n}   (y_k-ax_k-b)^2
$$
​	其中y为实际值，a、b分别为回归方程的斜率和截距。

​	令a=1，我们得出误差e。为了减小这个误差，我们试着将a的值增大到2，这里假设误差增大，继续增大参数的值没有意义，那么我们就减小它，注意到误差变小。

​	**至此，我们就完成了两次“反向传播”**：在参数初始化后开始计算（向前传播）的过程中，为了减小误差，回头进行增加或减小参数的值（反向传播），以达到减小误差的目的。

​	<font color="adssadasdasadasasdassdasddsssss">**在学习过程中，我发现了更有趣的理解“反向”概念的方法：**</font>

​	定义一个“神经单元误差”δ:
$$
\delta_{j}^{i}=\frac{\partial C}{\partial z_{j}^{i}} (i=1,2,3,...)
$$
​	其中C为损失函数，z为神经网络中关于某变量的函数（或者称其为关于偏置和权重的函数，其中偏置可以简单理解为上文的“截距”，权重则是“斜率”），i、j表示神经层中第i层、第j个神经单元。

​	神经单元误差就是关于z的偏导数，也就是关于所谓斜率和截距的偏导数。

​	（PS：主要是方便我自己理解，故下文均使用“斜率和截距”来代替“权重和偏置”，如果有理解错误的地方希望前辈们能指出，不要介意qwq）

​	沿用上文斜率a、截距b的定义，这里假设z是关于a、b的二次函数，自变量为x。我们得到
$$
\frac{\partial C}{\partial a^2}=\frac{\partial C}{\partial z^2}\frac{\partial z^2}{\partial a^2}
$$
​	显然，
$$
\frac{\partial z^2}{\partial a^2}=x
$$
​	也就是说，
$$
\frac{\partial C}{\partial a^2}=\delta ^2x
$$


​	通过同样的计算，我们可以得到
$$
\frac{\partial C}{\partial b^2}=\delta ^2
$$
​	扩展到一般形式：
$$
\frac{\partial C}{\partial a^i_j}=\delta _j^ix_j^{i-1},\frac{\partial C}{\partial b^i_j}=\delta _j^i
$$
​	通过对这两个式子进行变换，我们得到(这里省略了用偏导数将δ展开的过程)：
$$
\delta_j^i=\{\delta_1^{i+1}a_1^{i+1} +...+\delta_m^{i+1}a_m^{i+1}\}\frac{\partial a^2}{\partial z^2}
$$
​	**该式表示第i层与第i+1层的神经单元误差的递推关系。**

​	虽然这个一般式对于一个真正的神经网络还过于简陋（没有输入、中间、输出层），并且上下标也标得一团乱麻（这主要是我自己的原因😰），但是它很好地展现了神经单元误差的递推关系：在真正的神经网络中，通过这样的关系，我们只需要求出输出层的神经单元误差，其他的误差就不需要通过偏导数计算，大大节省了时间。

​	就好像不断从后往前推导——这就是“反向”。

​	也就是说，**反向传播法以梯度下降为基础，特点是将繁杂的导数计算替换为数列的递推关系式。**

------

### <font color="red">3.**泰勒公式**又是什么？</font>

​	**泰勒公式：高中数学的神级技巧**。
​	好吧，如果每道题都通过高中学习的知识来回答问题实在有些无趣和班门弄斧（在前辈们面前），况且我对它也没有什么高深的见解，不如直接开门见山：

​	假设函数在点a处有n+1阶连续导数，那么对于x在a附近的值，函数f(x)可表示为：
$$
f(x)=f(a)+\frac{f\prime(a)}{1!}(x-a)+\frac{f\prime \prime(a)}{2!}+...+\frac{f^{(n)}(a)}{n!}
$$
也即
$$
\sum_{i=0}^{n}\frac{f^{(i)}(a)}{i!}(x-a)^i
$$
~~（手打这俩公式用了10分钟，期间一直试图弄明白n阶导怎么在上面显示，后来发现只要弄个上标就行了😭）~~

​	它所表示的近似值与实际值有所偏差，在正式使用时需要加上其n阶余项。**泰勒公式是微积分逼近法的里程碑，具有很高的精确度，在研究函数极限和估计误差等方面也同样不可或缺。**

------

### <font color="red">4.矩阵和高中遇到的向量有什么联系？</font>

​	向量是一个有序数列，也可以说是一条有向线段，可以为**0**（**0代表零向量**）。

​	矩阵是一个元素按照矩形排列的矩形数组，可以表示多个向量或者多个线性方程，0矩阵即为所有元素都为0的矩阵。

​	我们可以将矩阵看作是行向量和列向量的组合形式，也可以将向量看作是一个特殊的矩阵。零向量和零矩阵也不例外。

以下是一个m*n矩阵：
$$
{A} = \left[\begin{array}{c} 
a_{11} & a_{12} & \dots & a_{1n} \\ 
a_{21} & a_{22} & \dots & a_{2n} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
a_{m1} & a_{m2} & \dots & a_{mn} 
\end{array}\right]
$$
​	其中每一行每一列都能看作是一个行向量/列向量：
$$
a = \begin{pmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1} \end{pmatrix}or \ a = (a_{11},a_{12},...,a_{1n}) 
$$
​	矩阵乘法在入门题中有所考察，这里不多赘叙。

------

### <font color="red">5.它们在机器学习中的作用？</font>

​	现在是总结时间！

1. ##### **偏导数、梯度**：<br>        <font color="blue">我的理解更偏向于它们是为了**梯度下降法**和**反向传播法**而服务的，主要作用是协助来训练机器学习中的模型，以达到最小化误差的目的。</font>

2. **梯度下降和反向传播：**<br>        <font color="blue">**正如上文所说，他们是计算最小误差（最小化损失/代价函数）的重要工具。高效的计算速度和广泛的适用性使得梯度下降和反向传播成为了机器学习中不可或缺的一部分，前者可用于参数的优化、更新，后者用于方便地计算梯度，使得多变量函数的预测成为可能。**</font><br>PS：其实我到现在为止才弄明白损失函数与代价函数的区别，按照上文的定义，我在第二题中写的C函数其实更偏向于代价函数😰

3. **泰勒公式：**<br><font color="blue">        **通过查阅资料，我了解到梯度计算不是万能的：在大规模的模型训练中，对损失函数进行梯度下降法时往往只利用了一阶导数来优化更新参数，这将导致较低的计算效率。**</font><br>        <font color="orange">**因此泰勒公式——作为一个拥有极高精确度的局部线性近似——被运用过来处理损失函数的二阶近似，利用二阶导来更快地找到最优值，这也就是二阶优化法。**</font>        <br>        **例如在神经网络中常用的sigmoid函数，它是在神经网络中具有代表性的激活函数:**
   $$
   \sigma(x)=\frac{1}{1+e^{-x}}
   $$
   <br>        （函数形式引用自《深度学习中的数学》）<br>	为了求得其在x=1处的近似值，通过泰勒展开式
   $$
   \sigma(x)\approx\frac{1}{2}+\frac{1}{4}-\frac{1}{48}=\frac{35}{48}
   $$
   ​	我们可以得到一个较为精确的近似。<br>不过由于阶数提高，计算复杂度也显著提高，计算成本也呈指数型增长。一般来说，一阶和二阶就已经足够解决问题，但更高阶的优化法在理论上也是可行的。

4. **矩阵和向量：**<br><font color="blue">        **向量和矩阵都是机器学习算法中的基础。从机器学习中numpy和pytorch关于矩阵的复杂运算和高频的使用场景来看，其重要性可见一斑。**</font><br>        机器学习中，数据点通常用向量来表示，而数据集可以看作是高维向量空间中的一组向量，向量组成了矩阵，而对矩阵的操作（例如矩阵乘法）则提供了更方便的数据表示和线性变换。

   <font color="orange">**继续使用线性回归来展示理解**：</font><br>	用矩阵A（m*n）表示输入数据，向量a表示权重（斜率）,用y表示预测结果。请出老朋友函数C：
   $$
   C=\sum_{k=1}^{n}   (实际值_k-ax_k-b)^2
   $$
   ​	其中函数
   $$
   y=ax+b
   $$
   ​	可以简单表示为
   $$
   y=Aa
   $$
   

   ​	这是由于矩阵乘法的特性，A@a得到一个有m行的列向量，其中的每一个数据都可以视作为对应样本的预测结果。**虽然最终结果是一个列向量，但是它表示所有样本的预测值，因此我们可以简单地写作这样的形式。**

   ​	最后，我们就可以将矩阵-向量乘法的线性变换与损失函数联系起来，通过这种方式，神经网络的每一层就能通过矩阵的运算来处理数据。